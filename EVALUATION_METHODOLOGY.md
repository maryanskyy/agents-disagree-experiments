# T-023: Evaluation Methodology & Bias Mitigation for Multi-Agent LLM Pipeline Outputs

**Author:** Elena Chen (PhD-1, Theoretical Track)  
**Date:** 2026-02-28  
**Status:** Deep Research Report  
**For:** "When Agents Disagree: Quorum-Based Consensus and Adaptive Orchestration Topology for Multi-Agent LLM Pipelines"

---

## Executive Summary

This report provides a comprehensive survey of evaluation methodology for multi-agent LLM pipelines, with a focus on LLM-as-judge evaluation, documented biases, mitigation strategies, and metric design. The central thesis is that **evaluation methodology is the load-bearing wall of our experiment**: if our evaluation protocol is flawed, our results on quorum-based consensus and adaptive orchestration are unpublishable regardless of how elegant the formal framework.

The report synthesizes findings from 30+ papers (2023--2026) and concludes with a concrete, step-by-step evaluation protocol designed to withstand hostile peer review. Key findings include: (1) LLM-as-judge achieves >80% agreement with humans for general quality but exhibits systematic biases that are *especially dangerous* for multi-agent comparison studies; (2) position bias, verbosity bias, self-preference bias, and overlap bias interact in ways that can systematically favor multi-agent outputs over single-agent outputs (or vice versa) if not controlled; (3) pairwise comparison with Bradley-Terry aggregation is strictly preferred over absolute scoring for our use case; and (4) a minimum of 15% human validation is required for publishable results.

---

## 1. LLM-as-Judge: State of the Art

### 1.1 Foundational Framework

The LLM-as-judge paradigm was formally established by **Zheng et al. (2023)** in "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena" (NeurIPS 2023 Datasets and Benchmarks Track, arXiv:2306.05685). Their key findings:

- **Agreement rates:** Strong LLM judges (GPT-4) achieve >80% agreement with human preferences, matching inter-human agreement levels.
- **Scalability:** LLM-as-judge provides a scalable, explainable proxy for human preferences that are otherwise prohibitively expensive.
- **Documented biases:** They identified three systematic biases: **position bias** (preference for the first or second response), **verbosity bias** (preference for longer responses), and **self-enhancement bias** (models favoring their own outputs).
- **Limited reasoning:** LLM judges struggle with tasks requiring deep mathematical reasoning or highly specialized domain knowledge.

The MT-Bench framework uses multi-turn questions across 8 categories (writing, roleplay, extraction, reasoning, math, coding, knowledge, stem), with evaluation on a 1--10 scale. Chatbot Arena complements this with crowdsourced pairwise battles using the Elo rating system.

### 1.2 Documented Biases: A Taxonomy

The literature now identifies **at least seven distinct bias types** in LLM-as-judge evaluation:

**1. Position Bias.** The most extensively studied bias. **Wang et al. (2023)** ("Large Language Models are not Fair Evaluators," arXiv:2305.17926) demonstrated that simply altering the order of candidate responses can flip evaluation outcomes: "Vicuna-13B could beat ChatGPT on 66 over 80 tested queries with ChatGPT as an evaluator" simply by manipulating position. **Xu et al. (2026)** (arXiv:2602.02219) extend this to rubric-based evaluation, showing that position bias also affects multi-choice rubric settings where LLMs prefer score options appearing at specific positions in the rubric list.

**2. Verbosity Bias.** LLM judges systematically prefer longer responses even when shorter ones are more accurate or appropriate (Zheng et al. 2023). This is especially dangerous for multi-agent evaluation because consensus outputs may be longer (aggregating multiple perspectives) than single-agent outputs.

**3. Self-Preference Bias.** **Lehr et al. (2025)** ("Extreme Self-Preference in Language Models," arXiv:2509.26464) conducted ~20,000 queries across 4 LLMs and found "massive self-preferences" -- models overwhelmingly paired positive attributes with their own names, companies, and CEOs. Critically, self-love "followed assigned, not true, identity," meaning it can be triggered by identity cues in the prompt. **Roytburg et al. (2026)** (arXiv:2601.22548) performed a critical sanity check, introducing an Evaluator Quality Baseline: when controlling for judges voting incorrectly on hard problems, only 51% of initial self-preference findings retained statistical significance. **Roytburg et al. (2025)** (arXiv:2509.03647) showed that steering vectors can reduce unjustified self-preference bias by up to 97%.

**4. Overlap/Style Bias.** **Fang et al. (2026)** ("Blind to the Human Touch," arXiv:2602.07673) showed that LLM judges "increasingly prefer summaries generated by other LLMs over those written by humans as the similarities between the judged summaries decrease." This reveals a fundamental bias toward LLM-stylistic patterns. When our multi-agent pipeline produces outputs that are "more LLM-like" (through iterative refinement/consensus), this bias could systematically inflate multi-agent scores.

**5. Overrating Bias.** **Yu et al. (2026)** ("When LLM Judges Inflate Scores," arXiv:2602.17170) showed that models "consistently assign inflated relevance scores -- often with high confidence -- to passages that do not genuinely satisfy the underlying information need." This is a system-wide bias toward leniency, making it harder to distinguish good from great outputs.

**6. Rubric-Induced Preference Drift (RIPD).** **Ding et al. (2026)** ("Rubrics as an Attack Surface," arXiv:2602.13576) identified that even "seemingly natural, criterion-preserving edits" to evaluation rubrics can "produce systematic and directional shifts in a judge's preferences," with accuracy reductions up to 27.9%. This means our rubric design is itself a potential source of bias.

**7. Unknown/Emergent Biases.** **Lai et al. (2026)** ("BiasScope," arXiv:2602.09383, accepted ICLR 2026) proposed an automated framework for discovering *unknown* biases. Their key finding: "even powerful LLMs as evaluators show error rates above 50% on JudgeBench-Pro," suggesting that our current understanding of LLM-judge biases is incomplete.

### 1.3 When LLM-as-Judge Fails

Based on the literature, LLM-as-judge is **unreliable** for:

1. **Mathematical reasoning verification** (Gonzalez et al. 2026, arXiv:2602.20629): LLM judges show a systematic "Alignment Gap" when evaluating mathematical proofs. Standard protocols are insufficient for verifying logical correctness.
2. **Tasks requiring factual grounding** where the judge lacks domain knowledge.
3. **Subtle quality distinctions** where outputs are close in quality (Yu et al. 2026 -- overrating makes fine-grained discrimination unreliable).
4. **Tasks where human creativity/style diverges from LLM norms** (Fang et al. 2026 -- overlap bias against human-like writing).
5. **Hard evaluation problems** where the judge's own capability is the bottleneck (Roytburg et al. 2026 -- self-preference compounds with evaluation difficulty).

### 1.4 Human--LLM Judge Correlation

The Zheng et al. (2023) benchmark established >80% agreement between GPT-4 judges and human preferences. However, this figure requires nuance:

- Agreement is highest for clear-cut quality differences and lowest for subtle distinctions.
- Agreement varies by task type: higher for factual/coding tasks, lower for creative/open-ended tasks.
- Agreement is measured on *single-agent* outputs; no established benchmark exists for *multi-agent consensus* outputs.
- The Chatbot Arena Elo system uses Bradley-Terry models for pairwise aggregation, which provides more robust rankings than raw win rates.

---

## 2. Evaluating Multi-Agent Outputs Specifically

### 2.1 The Multi-Agent Evaluation Challenge

Evaluating multi-agent pipeline outputs introduces unique challenges not present in standard single-model evaluation:

1. **Confounded production process.** A multi-agent output is the product of multiple inference calls, debate rounds, and consensus mechanisms. The judge evaluates the *output*, but we need to infer properties of the *process*.
2. **Length and detail asymmetry.** Multi-agent consensus outputs tend to be longer, more detailed, and more hedged than single-agent outputs. Without controlling for this, verbosity bias systematically favors multi-agent systems.
3. **Style homogenization.** Consensus mechanisms tend to smooth out distinctive voice and bold claims, producing outputs that are "safer" but potentially less creative. LLM judges may reward this conservatism.
4. **The Quorum Paradox measurement problem.** We need to detect cases where adding agent N+1 *hurts* quality. This requires fine-grained quality discrimination exactly where LLM judges are weakest.

### 2.2 Blind vs. Informed Evaluation

**Recommendation: Strictly blind evaluation.**

The judge must NOT know:
- How many agents produced the output
- What topology was used (flat, hierarchical, hybrid)
- What consensus mechanism was applied
- Whether the output is from a single agent or multi-agent pipeline

Rationale: **Choi et al. (2025)** ("When Identity Skews Debate," arXiv:2510.07517) demonstrated that identity information creates "identity-driven sycophancy and self-bias" in multi-agent debate. By analogy, if a judge knows an output was produced by a 5-agent quorum, it may subconsciously assign higher credibility. Their proposed Identity Bias Coefficient (IBC) formalizes this: agents (including judges) are prone to weighting responses based on perceived source identity rather than content.

The only exception: we may want a *separate* informed evaluation phase specifically to study whether knowledge of the production process affects quality judgments (itself a contribution).

### 2.3 Consensus Quality vs. Individual Quality

We need to evaluate at multiple levels:

1. **Individual agent responses** (before consensus) -- to establish baselines.
2. **Consensus output** (after the quorum mechanism) -- the final product.
3. **The delta** (consensus minus best individual) -- does consensus improve upon the best single agent?

This three-level evaluation is essential because:
- If consensus quality < best individual quality, that is evidence for the Quorum Paradox.
- If consensus quality > best individual quality, that is the "Disagreement Dividend."
- If consensus quality is approximately equal to mean individual quality, consensus is merely averaging.

### 2.4 Prior Work on Multi-Agent Evaluation

Several recent papers address evaluation in multi-agent settings:

**Hu et al. (2025)** ("Multi-Agent Debate for LLM Judges with Adaptive Stability Detection," arXiv:2510.12697) propose using multi-agent debate *within the judge itself* to improve evaluation quality. They formalize debate dynamics and prove that debate amplifies correctness compared to static ensembles. They introduce a stability detection mechanism using a time-varying Beta-Binomial mixture with adaptive stopping (Kolmogorov-Smirnov test). This is directly relevant -- we could use their framework for our judge panel.

**Cherian et al. (2025)** ("WISE: Weighted Iterative Society-of-Experts," arXiv:2512.02405) present a multi-agent debate framework with Solvers and Reflectors, using a modified Dawid-Skene algorithm for post-processing. Their evaluation methodology uses controlled-difficulty problem instances -- a design principle we should adopt (varying task difficulty systematically rather than relying on a fixed benchmark).

**Yao et al. (2025)** ("Roundtable Policy," arXiv:2509.16839) introduce a confidence-weighted consensus framework for multi-agent systems. Their evaluation uses heterogeneous scientific tasks and measures both reasoning quality and consensus stability -- relevant precedent for our experiment design.

---

## 3. Bias Mitigation Protocol

### 3.1 Position Randomization

**Mechanism:** For every pairwise comparison, randomize which output appears first (Position A) and which appears second (Position B). Evaluate each pair in *both* orderings. The final judgment is the majority across both orderings; if they disagree, record as a tie.

**Formal basis:** Wang et al. (2023) proposed "Balanced Position Calibration" -- aggregating results across orderings to determine final scores. Badshah et al. (2026) (SCOPE, arXiv:2602.13110) formalize this as "Bidirectional Preference Entropy (BPE)": query the judge under both response positions, aggregate implied preference probabilities to enforce invariance to response order, and convert to an entropy-based uncertainty score.

**Implementation:** For each pair (Output_A, Output_B), generate:
- Judgment_1: [A first, B second]
- Judgment_2: [B first, A second]
- If Judgment_1 and Judgment_2 agree: confident judgment
- If they disagree: flag as uncertain, escalate to human review or additional judge

### 3.2 Blind Evaluation

**Mechanism:** Strip all metadata from outputs before evaluation. The judge prompt receives:

```
Evaluate the following two responses to the given task.
Do not consider how the responses were generated.
Focus only on the quality of the content.

[Task Description]
[Response 1]: {anonymized output}
[Response 2]: {anonymized output}
```

**Additional controls:**
- Normalize output length by truncating to a maximum word count OR instructing the judge to explicitly disregard length differences.
- Remove any formatting artifacts that might hint at multi-agent provenance (e.g., numbered perspectives, synthesis headers, attribution markers).

### 3.3 Multi-Judge Agreement

**Mechanism:** Use at least 3 different judge models from at least 2 different model families.

**Recommended panel:**
- Judge 1: GPT-4-class model (OpenAI family)
- Judge 2: Claude-3.5-class model (Anthropic family)
- Judge 3: Gemini-class model or open-weight model (Google/Meta family)

**Rationale:** Qian et al. (2026) ("Who can we trust? LLM-as-a-jury," arXiv:2602.16610) show that LLM judges "vary substantially in performance across tasks and aspects, and their judgment probabilities may be biased and inconsistent." They propose BT-sigma, a judge-aware extension of the Bradley-Terry model that introduces a **discriminator parameter** for each judge to jointly infer item rankings and judge reliability from pairwise comparisons alone. The learned discriminator "strongly correlates with independent measures of cycle consistency of LLM judgments."

**Inter-rater reliability measurement:** Following Jain et al. (2025) (arXiv:2512.20352), use dual reliability metrics:
- **Cohen's Kappa** for categorical agreement between judge pairs. Target: kappa > 0.60 (substantial agreement). If kappa < 0.40, the evaluation dimension is unreliable and should be redesigned or dropped.
- **Semantic similarity** (cosine similarity of judge rationales using embedding models) for continuous agreement on open-ended evaluations. Target: >85%.

### 3.4 Calibration with Reference Outputs

**Mechanism:** Include known-quality anchor outputs in every evaluation batch.

**Design:**
1. Create a calibration set of ~20 tasks with human expert-rated outputs at known quality levels (1-star, 3-star, 5-star).
2. Interleave these calibration items randomly among real experimental outputs.
3. Monitor each judge's calibration accuracy: does it correctly rank the anchor outputs?
4. Use calibration accuracy to weight judges (poorly calibrated judges get lower weight in aggregation).

**Formal model:** This implements the "Multiple Evidence Calibration" strategy from Wang et al. (2023), extended with a Bayesian judge-quality estimation: if Judge_j correctly orders 18/20 calibration items, its weight in the BT-sigma model is proportional to its calibration accuracy.

### 3.5 Human Validation

**Minimum viable human evaluation:**
- **15% random sample** of all LLM-judged pairs should be independently evaluated by human raters.
- **100% human evaluation** for items where LLM judges disagree (uncertain items from the position randomization step).
- **100% human evaluation** for the Quorum Paradox cases (where adding agents appears to decrease quality) -- these are our most important claims and must be human-verified.

**Human rater requirements:**
- At least 2 human raters per item for inter-rater reliability.
- Human raters should be blind to experimental condition (same blind protocol as LLM judges).
- Report human--human agreement (Cohen's kappa) and human--LLM agreement separately.

**Estimated effort:** For an experiment with 100 tasks x 5 topologies = 500 output pairs, 15% sample = 75 pairs needing human evaluation. With 2 raters and ~5 min/pair, this is ~12.5 person-hours -- feasible.

---

## 4. Metric Design

### 4.1 Analytical Reasoning Metrics

For tasks involving logic, argument analysis, and structured reasoning:

| Metric | Description | Scale | Weight |
|--------|-------------|-------|--------|
| **Correctness** | Is the final answer/conclusion correct? | Binary (0/1) | 0.30 |
| **Reasoning Depth** | Number of valid inferential steps; does the response address edge cases? | 1--5 | 0.25 |
| **Evidence Quality** | Are claims supported by specific evidence, citations, or logical premises? | 1--5 | 0.20 |
| **Logical Coherence** | Is the argument internally consistent? Are there contradictions? | 1--5 | 0.15 |
| **Conciseness** | Does the response avoid irrelevant tangents while being complete? | 1--5 | 0.10 |

**Rubric anchoring:** Each scale point (1--5) must be defined with concrete examples. Example for Reasoning Depth:
- 1: Surface-level answer with no reasoning shown
- 2: One inferential step, no consideration of alternatives
- 3: Multiple steps, basic consideration of counterarguments
- 4: Multi-step reasoning with edge cases addressed
- 5: Comprehensive reasoning covering all major angles, identifying unstated assumptions

### 4.2 Creative Generation Metrics

For stories, brainstorming, and poetry:

| Metric | Description | Scale | Weight |
|--------|-------------|-------|--------|
| **Originality** | Does the output avoid cliches and offer genuinely novel ideas or perspectives? | 1--5 | 0.25 |
| **Coherence** | Does the output maintain internal consistency and logical flow? | 1--5 | 0.20 |
| **Engagement** | Is the output compelling, surprising, or emotionally resonant? | 1--5 | 0.20 |
| **Craft** | Quality of language use: imagery, rhythm, word choice, structure | 1--5 | 0.20 |
| **Task Fidelity** | Does the output satisfy the constraints and requirements of the prompt? | 1--5 | 0.15 |

**Critical caveat for creative evaluation:** LLM judges are *least* reliable for creative tasks (Fang et al. 2026). Overrating bias is strongest here. **Recommendation:** Increase human validation to 25% for creative tasks and require human evaluation for any creative task where the experimental result is "counterintuitive" (i.e., supports the Quorum Paradox or Disagreement Dividend).

### 4.3 Composite vs. Separate Dimensions

**Recommendation: Report BOTH.**

- **Separate dimension scores** for transparency and to identify *which* quality dimensions benefit (or suffer) from multi-agent consensus. This is essential for understanding *why* a topology works.
- **Composite score** (weighted average) for headline comparisons and statistical tests. The composite enables power analysis and reduces multiple-comparison burden.

The weights should be validated empirically: compute composite scores with several weighting schemes and check that the ranking of topologies is stable across schemes (sensitivity analysis). If rankings change significantly with weights, report separate dimensions only.

### 4.4 Measuring Disagreement

Disagreement among agents in a quorum is itself a key variable. Metrics:

**4.4.1 Vote Entropy.** For discrete classification tasks where agents vote:

$$H(V) = -\sum_{c \in C} p(c) \log_2 p(c)$$

where $p(c)$ is the fraction of agents voting for class $c$. Maximum entropy = maximum disagreement. This is interpretable and principled.

**4.4.2 Pairwise Semantic Similarity.** For open-ended generation tasks:

$$\text{Disagreement}(Q) = 1 - \frac{2}{n(n-1)} \sum_{i<j} \cos(\mathbf{e}_i, \mathbf{e}_j)$$

where $\mathbf{e}_i$ is the embedding (e.g., from a sentence-transformer) of agent $i$'s response. Range [0, 1]; 0 = perfect agreement, 1 = maximum disagreement.

**4.4.3 Lexical Diversity.** Complement semantic similarity with lexical diversity:

$$\text{LexDiv}(Q) = 1 - \frac{|\bigcap_i \text{tokens}(r_i)|}{|\bigcup_i \text{tokens}(r_i)|}$$

This captures surface-level disagreement vs. semantic disagreement.

**4.4.4 Rank Correlation (Kendall's tau).** For tasks where agents produce rankings or ordered lists, measure pairwise Kendall's tau between agent rankings.

### 4.5 Measuring the Quorum Paradox

The Quorum Paradox -- "adding agent N+1 hurts quality" -- requires a metric that definitively captures **monotonicity violation** in the quality-vs-agent-count function.

**Definition (Quorum Paradox Index -- QPI):**

Let $Q(n)$ be the mean quality score when using a quorum of size $n$. The Quorum Paradox Index for adding the $(n+1)$-th agent is:

$$\text{QPI}(n) = Q(n) - Q(n+1)$$

If $\text{QPI}(n) > 0$, adding the $(n+1)$-th agent decreased quality. We need:
1. $\text{QPI}(n) > 0$ (quality decreased)
2. Statistical significance: $\text{QPI}(n)$ differs from 0 at $p < 0.05$ (paired test across tasks)
3. Effect size: Cohen's $d > 0.2$ (at least small effect)

**Stronger measure -- Cumulative Quality Curve:**

Plot $Q(n)$ vs. $n$ for each topology and task type. The Quorum Paradox is visible as a non-monotone region. Fit a piecewise model:
- $Q(n) = a \cdot \log(n) + b$ for the increasing phase
- Identify the inflection point $n^*$ where $Q'(n^*) = 0$

The MVQ (Minimum Viable Quorum) is the smallest $n$ achieving $Q(n) \geq \theta$ for quality threshold $\theta$. The Paradox is formally: $\exists n > n^* : Q(n) < Q(n^*)$.

**Statistical robustness:** Use bootstrap confidence intervals for $Q(n)$ at each $n$. The Quorum Paradox is confirmed only if the confidence intervals for $Q(n)$ and $Q(n+1)$ do not overlap in the predicted direction (or equivalently, if a bootstrap test rejects $H_0: Q(n+1) \geq Q(n)$).

---

## 5. Alternative Evaluation Approaches

### 5.1 Pairwise Comparison vs. Absolute Scoring vs. Ranking

| Approach | Pros | Cons | For Our Use Case |
|----------|------|------|------------------|
| **Pairwise (Bradley-Terry)** | Eliminates absolute calibration issues; judges only compare two items at a time; well-founded statistical model | Quadratic in number of items; requires many comparisons | **Strongly recommended** |
| **Absolute scoring (1--10)** | Simple; direct; each item scored independently | Calibration drift; scale meaning varies between judges; overrating bias | Use only with calibration anchors |
| **Ranking (full ordering)** | Captures relative quality efficiently | Hard for judges to maintain global consistency; cognitive load | Not recommended for LLM judges |

**Recommendation:** Use **pairwise comparison as the primary evaluation mode**, with Bradley-Terry aggregation to produce global rankings. This follows the Chatbot Arena methodology (Zheng et al. 2023) and recent theoretical advances (Qian et al. 2026, Badshah et al. 2026).

Specifically, adopt the **BT-sigma model** from Qian et al. (2026): this jointly infers item rankings and judge reliability, providing an unsupervised calibration mechanism that improves aggregation by modeling judge reliability.

For supplementary analysis, also collect absolute scores (1--5 per dimension) with calibration anchors. This provides dimension-level detail that pairwise comparison alone cannot.

### 5.2 Rubric-Based vs. Holistic Evaluation

**Rubric-based evaluation** (scoring each dimension separately) is preferred for:
- Analytical reasoning tasks (where correctness is separable from explanation quality)
- Understanding *which* quality dimensions benefit from multi-agent consensus

**Holistic evaluation** (single overall quality judgment) is preferred for:
- Creative tasks where quality is emergent and holistic
- Final-headline comparisons across topologies

**Recommendation:** Use rubric-based evaluation as the primary mode (separate dimension scores), with a holistic "overall quality" judgment as an additional data point. Test whether holistic scores correlate with the composite of rubric scores (they should; if they diverge significantly, investigate).

**Rubric design caution:** Per Ding et al. (2026), rubrics are an attack surface. Keep rubrics simple, test them on calibration data, and report rubric-sensitivity analysis (vary rubric wording slightly and check if results change).

### 5.3 Reference-Based Metrics

| Metric | What It Measures | Suitable For | Limitations |
|--------|-----------------|-------------|-------------|
| **ROUGE** | N-gram overlap with reference | Summarization-like tasks with ground truth | Surface-level; penalizes paraphrasing; irrelevant for open-ended tasks |
| **BERTScore** | Semantic similarity to reference | Tasks with a known-good reference | Requires reference; biased toward BERT's training distribution |
| **BARTScore** | Generative probability of reference given candidate | Evaluation and generation tasks | Moderate correlation with human judgments; reference-dependent |

**For our use case:** Reference-based metrics are **not suitable as primary evaluation** because:
1. Our tasks are open-ended (analytical reasoning and creative generation have no single correct reference).
2. Multi-agent outputs may be valid but *differently valid* from any reference.
3. Reference-based metrics penalize diversity, which is exactly what we want to measure.

**However:** Use BERTScore as a *supplementary* metric to measure similarity between individual agent outputs and the consensus output. This quantifies how much the consensus mechanism changes the content.

### 5.4 Minimum Viable Human Evaluation

Based on the literature and our specific needs:

| Condition | % Human Eval | Rationale |
|-----------|-------------|-----------|
| Random sample (analytical) | 15% | Calibrates LLM--human agreement |
| Random sample (creative) | 25% | LLM judges less reliable for creative tasks |
| LLM judge disagreement | 100% | Uncertain items need human ground truth |
| Quorum Paradox cases | 100% | Most important claims must be human-verified |
| Calibration anchors | 100% | By definition, these have human ground truth |

**Total estimated human effort:** For 100 tasks x 5 topologies x 2 task types = 1000 evaluations:
- 15% random analytical: 75 pairs x 2 raters = 150 ratings
- 25% random creative: 125 pairs x 2 raters = 250 ratings
- Estimated disagreement (~15%): 150 pairs x 2 raters = 300 ratings
- Quorum Paradox flagged (~10%): 100 pairs x 2 raters = 200 ratings
- **Total: ~900 human ratings, ~75 person-hours at 5 min/rating**

This is feasible for a PhD-level experiment. Non-negotiable for publication.

---

## 6. What Could Go Wrong: Threat Model

### 6.1 Threats to Validity

| Threat | Severity | How It Manifests | Mitigation |
|--------|----------|-----------------|------------|
| **Position bias inflates/deflates specific conditions** | HIGH | If multi-agent outputs are systematically placed first, they get position advantage | Balanced position randomization (Section 3.1) |
| **Verbosity bias favors multi-agent outputs** | HIGH | Consensus outputs tend to be longer, verbosity bias favors them, inflated multi-agent scores | Length normalization + explicit judge instruction to ignore length + human validation |
| **Self-preference confounds** | MEDIUM | If Judge Model X was also one of the pipeline agents, it may prefer its own outputs | Ensure judge models are NEVER used as pipeline agents. Separate model pools. |
| **Overlap/style bias** | MEDIUM | Multi-agent consensus may produce "more LLM-like" text, LLM judges prefer it | Human validation subset; report human--LLM agreement delta by condition |
| **Overrating collapses quality distinctions** | HIGH | If judges give everything 4--5/5, we cannot detect the Quorum Paradox | Pairwise comparison (forces discrimination); calibration anchors include deliberately low-quality items |
| **Rubric sensitivity** | MEDIUM | Slight changes in rubric wording change results | Rubric sensitivity analysis; keep rubrics simple and pre-tested |
| **Insufficient power for Quorum Paradox** | HIGH | Effect may be small; with 100 tasks we may not detect it | Power analysis a priori; consider 200 tasks if effect size is < 0.3 |
| **Task selection bias** | MEDIUM | Results may not generalize beyond chosen tasks | Diverse task set; report results by task category; discuss generalizability |
| **Cherry-picking topologies** | LOW | Reviewer suspicion that we only tested topologies that work | Pre-register experimental conditions; report ALL results including null/negative |

### 6.2 Hostile Reviewer Attacks

A hostile (but fair) reviewer would attack:

1. **"Your LLM judges are biased toward multi-agent outputs because they are longer."**
   - Defense: Length-controlled analysis (bin results by output length); position randomization; human validation.

2. **"You used GPT-4 as both a pipeline agent and a judge."**
   - Defense: Strict separation of agent and judge model pools. Document this explicitly.

3. **"Your sample size is too small for the claims."**
   - Defense: Power analysis; bootstrap confidence intervals; effect sizes with confidence intervals.

4. **"The Quorum Paradox effect may be an artifact of evaluation noise."**
   - Defense: 100% human validation of Paradox cases; inter-rater reliability; replication across task types.

5. **"You only tested N=3,5,7 agents. The Paradox might disappear at N=9."**
   - Defense: Test a reasonable range; acknowledge limitations; frame as establishing the phenomenon, not exhaustive characterization.

6. **"Your rubrics are biased toward a particular kind of quality that favors consensus."**
   - Defense: Rubric sensitivity analysis; multiple rubric variants; human meta-evaluation of rubric fairness.

7. **"LLM-as-judge is insufficient for creative tasks."**
   - Defense: 25% human validation for creative tasks; report LLM--human agreement by task type; acknowledge limitations.

### 6.3 What Would Make Results Non-Publishable

1. **LLM judges agree with each other but NOT with humans** (inter-judge kappa > 0.8 but judge--human kappa < 0.4). This would indicate systematic shared bias, not reliability.
2. **Results reverse under different judge models.** If GPT-4 says Topology A wins but Claude says Topology B wins, results are judge-dependent and not generalizable.
3. **Quorum Paradox claims fail human validation.** If human raters disagree with LLM judges on the key Paradox findings, the central claim collapses.
4. **Insufficient statistical power.** If confidence intervals are too wide to distinguish topologies, we have not demonstrated anything.
5. **Evidence of confounding bias that we did not control for.** If a reviewer identifies a systematic bias that our protocol missed.

---

## 7. Recommended Evaluation Protocol: Step-by-Step

### Phase 0: Pre-Registration and Calibration (Before Running Experiments)

**Step 0.1: Pre-register experimental conditions.**
- Document all topologies, agent counts, consensus mechanisms, and task types BEFORE running experiments.
- Commit to analyzing ALL conditions, not just those that "work."

**Step 0.2: Design evaluation rubrics.**
- Draft rubrics for analytical and creative tasks (Sections 4.1, 4.2).
- Pilot rubrics on 10 calibration tasks with 2 human raters.
- Compute human--human kappa; iterate rubric until kappa > 0.60.

**Step 0.3: Build calibration set.**
- Create 20 tasks (10 analytical, 10 creative) with outputs at known quality levels (1/3/5).
- Verify with 3 human raters that quality levels are correctly ordered.

**Step 0.4: Select judge models.**
- Primary panel: 3 models from at least 2 families (e.g., GPT-4o, Claude 3.5, Gemini Pro).
- Verify: NO judge model is used as a pipeline agent.

**Step 0.5: Power analysis.**
- Estimate expected effect sizes from pilot data.
- Determine minimum number of tasks needed for 80% power at alpha = 0.05.

### Phase 1: Experimental Output Collection

**Step 1.1:** Run all pipeline configurations (topologies x agent counts x consensus mechanisms) on all tasks.

**Step 1.2:** Collect individual agent responses (before consensus) AND consensus outputs (after consensus).

**Step 1.3:** Anonymize all outputs -- strip metadata, normalize formatting, remove any multi-agent provenance signals.

### Phase 2: LLM-as-Judge Evaluation

**Step 2.1: Pairwise comparison (primary evaluation).**
- For each task, construct all relevant pairwise comparisons:
  - Single-agent (best baseline) vs. each multi-agent topology
  - Multi-agent topology A vs. topology B (for all pairs)
  - Quorum size N vs. N+1 (for all consecutive sizes)
- For each pair, evaluate in BOTH orderings (position randomization).
- Collect from ALL 3 judge models.

**Step 2.2: Rubric-based scoring (supplementary evaluation).**
- Score each output on all rubric dimensions (Section 4.1 for analytical, Section 4.2 for creative).
- Include calibration anchors in every batch.
- Use balanced permutation strategy for rubric position bias (Xu et al. 2026).

**Step 2.3: Monitor judge calibration.**
- Track each judge's accuracy on calibration items.
- If a judge's calibration accuracy drops below 70%, flag and investigate.

### Phase 3: Inter-Rater Reliability Analysis

**Step 3.1:** Compute pairwise Cohen's kappa between all judge pairs for each evaluation dimension.

**Step 3.2:** Compute BT-sigma model (Qian et al. 2026) to jointly estimate item rankings and judge reliability.

**Step 3.3:** Flag items where judges disagree; route to human evaluation.

**Step 3.4:** Acceptance criteria:
- Inter-judge kappa > 0.50 for all dimensions: proceed
- Inter-judge kappa 0.40--0.50: investigate; consider dropping dimension
- Inter-judge kappa < 0.40: dimension is unreliable; redesign or drop

### Phase 4: Human Validation

**Step 4.1:** Random sample: 15% analytical, 25% creative; blind human pairwise evaluation (2 raters each).

**Step 4.2:** All judge-disagreement items: human evaluation (2 raters each).

**Step 4.3:** All Quorum Paradox candidate items: human evaluation (2 raters each).

**Step 4.4:** Compute:
- Human--human kappa (should be > 0.60)
- Human--LLM-judge agreement (should be > 0.70; report per judge model)
- Disagreement rate between LLM and human on Quorum Paradox items specifically

**Step 4.5:** If human--LLM agreement < 0.60 for any major finding, **do not report that finding based on LLM evaluation alone.** Either expand human evaluation or caveat heavily.

### Phase 5: Aggregation and Analysis

**Step 5.1:** Aggregate pairwise comparisons using Bradley-Terry model with judge-quality weights (BT-sigma).

**Step 5.2:** Compute quality curves $Q(n)$ for each topology and task type.

**Step 5.3:** Test for Quorum Paradox:
- Bootstrap CI for QPI(n) at each n
- Statistical significance: paired permutation test
- Effect size: Cohen's d
- Human validation of all significant Paradox instances

**Step 5.4:** Compute disagreement metrics (Section 4.4) for each quorum.

**Step 5.5:** Test correlation between disagreement and quality (key theoretical prediction).

### Phase 6: Sensitivity Analysis and Reporting

**Step 6.1: Rubric sensitivity.** Repeat evaluation with 2 rubric variants (minor wording changes). Report stability of rankings.

**Step 6.2: Judge sensitivity.** Report results for each judge model separately AND aggregated. Flag any result that depends on a single judge.

**Step 6.3: Length analysis.** Report output length by condition. Run length-controlled analysis (compare outputs within same length quartile).

**Step 6.4: Full disclosure.** Report ALL results including null findings and failed conditions. Present aggregate tables AND individual task-level scatter plots.

---

## 8. Summary: Threat-Mitigation Matrix

| Bias / Threat | Detection Method | Mitigation Strategy | Validation |
|---------------|-----------------|---------------------|------------|
| Position bias | Bidirectional evaluation | Balanced position randomization | Check that both-order agreement > 90% |
| Verbosity bias | Length correlation analysis | Length normalization; explicit judge instruction | Length-controlled subgroup analysis |
| Self-preference | Ensure judge is not agent model | Strict model pool separation | Report model pools explicitly |
| Overlap/style bias | Compare LLM--human agreement by condition | Human validation; diversity of judge families | Human--LLM delta should not correlate with condition |
| Overrating | Calibration anchor tracking | Pairwise comparison (forces discrimination) | Judge accuracy on calibration items > 70% |
| Rubric sensitivity | Multi-variant rubric testing | Simple rubrics; pre-testing | Rankings stable across rubric variants |
| Low power | A priori power analysis | Sufficient task count; bootstrap CIs | CIs exclude zero for significant claims |
| Inter-judge disagreement | Cohen's kappa computation | BT-sigma model; drop unreliable dimensions | kappa > 0.50 for all reported dimensions |
| Quorum Paradox artifacts | Human validation of key cases | 100% human eval for Paradox items | Human raters confirm > 75% of LLM Paradox findings |

---

## 9. References

1. Zheng, L. et al. (2023). "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena." NeurIPS 2023. arXiv:2306.05685.
2. Wang, P. et al. (2023). "Large Language Models are not Fair Evaluators." arXiv:2305.17926.
3. Lai, P. et al. (2026). "BiasScope: Towards Automated Detection of Bias in LLM-as-a-Judge Evaluation." ICLR 2026. arXiv:2602.09383.
4. Ding, R. et al. (2026). "Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges." arXiv:2602.13576.
5. Yang, B. et al. (2026). "FairJudge: An Adaptive, Debiased, and Consistent LLM-as-a-Judge." arXiv:2602.06625.
6. Badshah, S. et al. (2026). "SCOPE: Selective Conformal Optimized Pairwise LLM Judging." arXiv:2602.13110.
7. Qian, M. et al. (2026). "Who can we trust? LLM-as-a-jury for Comparative Assessment." arXiv:2602.16610.
8. Yu, C. et al. (2026). "When LLM Judges Inflate Scores: Exploring Overrating in Relevance Assessment." arXiv:2602.17170.
9. Fang, J. et al. (2026). "Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation." arXiv:2602.07673.
10. Xu, Y. et al. (2026). "Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge." arXiv:2602.02219.
11. Gonzalez, S. et al. (2026). "QEDBENCH: Quantifying the Alignment Gap in Automated Evaluation of University-Level Mathematical Proofs." arXiv:2602.20629.
12. Xia, X. et al. (2026). "A Statistical Framework for Alignment with Biased AI Feedback." arXiv:2602.08259.
13. Bosnjakovic, D. (2026). "The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias." arXiv:2602.17127.
14. Lehr, S.A. et al. (2025). "Extreme Self-Preference in Language Models." arXiv:2509.26464.
15. Roytburg, D. et al. (2025). "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators." arXiv:2509.03647.
16. Roytburg, D. et al. (2026). "Are LLM Evaluators Really Narcissists? Sanity Checking Self-Preference Evaluations." arXiv:2601.22548.
17. Hu, T. et al. (2025). "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection." arXiv:2510.12697.
18. Choi, H.K. et al. (2025). "When Identity Skews Debate: Anonymization for Bias-Reduced Multi-Agent Reasoning." arXiv:2510.07517.
19. Cherian, A. et al. (2025). "WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate." arXiv:2512.02405.
20. Yao, Y. et al. (2025). "Roundtable Policy: Confidence-Weighted-Consensus Aggregation Improves Multi-Agent-System Reasoning." arXiv:2509.16839.
21. Dadkhahi, H. et al. (2025). "Distribution-Calibrated Inference Time Compute for Thinking LLM-as-a-Judge." arXiv:2512.03019.
22. Jain, N. et al. (2025). "Multi-LLM Thematic Analysis with Dual Reliability Metrics." arXiv:2512.20352.
23. Cho, A. (2025). "GrandJury: A Collaborative Machine Learning Model Evaluation Protocol for Dynamic Quality Rubrics." arXiv:2508.02926.
24. Feng, Y. et al. (2025). "Are We on the Right Way to Assessing LLM-as-a-Judge?" arXiv:2512.16041.
25. Xu, M. et al. (2026). "A Judge-Aware Ranking Framework for Evaluating Large Language Models without Ground Truth." arXiv:2601.21817.
26. Aggarwal, S. et al. (2026). "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability." arXiv:2602.17544.
27. Chang, E.Y. and Chang, E.Y. (2025). "MACI: Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning." arXiv:2510.04488.
28. Bhat, N. et al. (2025). "Creativity Benchmark: A Benchmark for Marketing Creativity for Large Language Models." arXiv:2509.09702.
29. Jafari, K. et al. (2026). "Expert Evaluation and the Limits of Human Feedback in Mental Health AI Safety Testing." arXiv:2601.18061.
30. Ballon, M. et al. (2025). "Estimating Problem Difficulty without Ground Truth Using Large Language Model Comparisons." arXiv:2512.14220.

---

## 10. Appendix: Quick-Reference Decision Tree

```
Is there a single correct answer?
|-- YES --> Use correctness as primary metric; LLM judge as secondary
+-- NO --> Is the task analytical or creative?
    |-- ANALYTICAL --> Rubric-based evaluation + pairwise comparison
    |   |-- Primary: LLM judge panel (3 models) with position randomization
    |   |-- Secondary: Dimension-level rubric scores
    |   |-- Human validation: 15% random + 100% disagreement items
    |   +-- Aggregation: BT-sigma model
    +-- CREATIVE --> Holistic + rubric evaluation + pairwise comparison
        |-- Primary: LLM judge panel (3 models) with position randomization
        |-- Secondary: Dimension-level rubric scores + holistic score
        |-- Human validation: 25% random + 100% disagreement items
        +-- Aggregation: BT-sigma model with higher human-weight calibration
```

---

*This report should be treated as the methodological foundation for all experimental evaluation in the paper. Any deviation from this protocol should be documented and justified. The protocol is designed to survive hostile peer review by addressing every known bias with a specific, documented mitigation strategy.*
