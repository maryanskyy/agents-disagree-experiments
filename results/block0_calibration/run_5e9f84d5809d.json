{
  "run_id": "run_5e9f84d5809d",
  "block_id": "block0_calibration",
  "status": "ok",
  "timestamp_utc": "2026-03-01T03:47:40.335617+00:00",
  "config": {
    "task_type": "analytical",
    "task_id": "analytical_05_statistical_claim_flaw",
    "topology": "flat",
    "consensus": "simple_vote",
    "agent_count": 1,
    "model_assignment": [
      "claude-opus-4-6"
    ],
    "disagreement_level": 1,
    "temperature": 0.3,
    "prompt_strategy": "identical",
    "repetition": 3,
    "quality_threshold": null,
    "posthoc_quality_thresholds": []
  },
  "task": {
    "title": "Diagnose Flaws in a Statistical Performance Claim",
    "prompt": "A startup claims: \"Our AI tutor improves exam scores by 25%.\" Their evidence: 40 volunteer students used the tutor for two weeks and their average score rose from 64 to 80. There was no control group. Students self-selected into the trial. Exams before and after were not equated for difficulty. The company highlights p < 0.05 from a paired t-test and concludes broad effectiveness.\n\nCritically evaluate this claim. Identify methodological threats to validity, explain why the statistical significance result is insufficient for the causal claim, and propose a minimally credible redesign that could be executed within one school term. Include sampling, randomization (or quasi-experimental fallback), outcome measurement, and pre-registration elements. End by stating what claim is actually justified from current evidence.\n",
    "rubric": [
      "Flags core validity issues (selection bias, no control, test non-equivalence, regression effects).",
      "Explains mismatch between p-value and causal inference.",
      "Proposes feasible improved study design with concrete safeguards.",
      "Final justified claim is appropriately modest and evidence-aligned."
    ]
  },
  "outputs": [
    {
      "agent_id": "agent_0",
      "model_name": "claude-opus-4-6",
      "text": "# Critical Evaluation of the AI Tutor Effectiveness Claim\n\n## 1. Methodological Threats to Validity\n\n### Threats to Internal Validity (Can we infer causation at all?)\n\n| Threat | Explanation |\n|---|---|\n| **Selection bias (self-selection)** | Volunteers are systematically different from the general student population. They are likely more motivated, more tech-savvy, or more anxious about upcoming exams\u2014all of which independently predict score improvement. The \"treatment effect\" is confounded with the traits that drove participation. |\n| **No control group** | Without a comparison group experiencing the same time period, maturation, and testing conditions, every co-occurring influence is an alternative explanation. Any improvement could be entirely due to factors unrelated to the tutor. |\n| **History / maturation** | Two weeks of ordinary studying, classroom instruction, peer discussion, and approaching-exam motivation would produce some score gain regardless of the tutor. This is indistinguishable from a tutor effect in this design. |\n| **Testing effects (practice / sensitization)** | Taking the pre-test itself is a learning event. Students become familiar with format, question types, and their own knowledge gaps. The post-test improvement partly reflects this familiarity. |\n| **Non-equivalent test forms** | The pre- and post-exams were not equated for difficulty. If the post-exam was even slightly easier, the 16-point gain is artifactually inflated. Without item-response-theory equating or a common anchor set, the two scores are not on the same scale, making the difference literally uninterpretable as a gain. |\n| **Regression to the mean** | A starting average of 64 suggests many students scored below their true ability on the pre-test (bad day, unfamiliarity, low stakes). Their scores would naturally regress upward on any retest, with or without intervention. Self-selected students who feel they are \"underperforming\" are especially susceptible to this. |\n| **Hawthorne / novelty effect** | Participants knew they were in a trial of a new product. Increased attention, effort, and engagement from novelty alone can boost performance temporarily and would vanish at scale. |\n| **Demand characteristics** | Students may have tried harder on the post-test to validate the tool they volunteered to use, or to please the researchers/company. |\n| **Attrition (unreported)** | We are told 40 students participated, but not how many initially enrolled. If weaker students dropped out, the post-test average is inflated by survivorship bias. |\n\n### Threats to External Validity (Can we generalize?)\n\n- **Sample**: 40 volunteers from (presumably) one institution, one subject, one age group. No basis for \"broad effectiveness.\"\n- **Dosage ambiguity**: \"Used the tutor for two weeks\" says nothing about hours of engagement, which modules were used, or compliance. Effect cannot be linked to a replicable treatment protocol.\n- **Ecological validity**: A short trial with motivated volunteers under observation does not predict real-world adoption where students use the tool sporadically and without monitoring.\n\n### Threats to Construct Validity\n\n- The construct \"improves exam scores\" conflates genuine learning with test-taking strategy, memorization of specific content, or simply increased study time. Without measuring transfer, retention, or deeper understanding, we don't know what changed.\n\n---\n\n## 2. Why p < 0.05 Is Insufficient for the Causal Claim\n\nThe paired t-test answers exactly one question: *\"Is the mean difference between pre- and post-scores unlikely to be zero, assuming the difference scores are drawn from a normal distribution?\"* It says nothing about **why** the difference exists.\n\n### Specific insufficiencies:\n\n1. **Statistical significance \u2260 causal identification.** The p-value quantifies sampling variability, not confounding. A paired t-test controls for between-subject variance but controls for zero confounds. Every threat listed above (history, maturation, regression, testing effects, non-equivalent forms) produces a real, non-zero mean difference that the t-test will faithfully detect. The test is doing its job correctly\u2014it's just being asked the wrong question.\n\n2. **p < 0.05 with n = 40 and a 16-point gain is unsurprising and uninformative.** A 16-point mean increase on a ~100-point scale is large. Even with substantial within-subject variability (SD of differences ~20), the t-statistic would be approximately 16/(20/\u221a40) \u2248 5.06, yielding p \u226a 0.001. The \"significance\" here is trivially achieved and tells us only that *something* changed\u2014which no one doubts. The question is *what*.\n\n3. **No effect size contextualization.** The company does not report Cohen's d, does not benchmark against typical score gains over two weeks of normal study, and does not compare to alternative interventions. A 25% relative gain sounds impressive but is uninterpretable without a counterfactual.\n\n4. **Multiple implicit comparisons.** If the company tested multiple cohorts, subjects, or time windows and reported only the most favorable result, the p-value is inflated by selection. No pre-registration is mentioned.\n\n5. **Violation of assumptions.** With non-equivalent test forms, the pre-post difference is measured in incomparable units. Running a t-test on scores from two different scales is like subtracting kilograms from pounds and testing whether the difference is zero\u2014the arithmetic is valid but the inference is meaningless.\n\n**Bottom line:** The p-value certifies that the observed difference is unlikely under the null hypothesis of zero change. But the null hypothesis of zero change was never the interesting hypothesis. The interesting hypothesis is that the *tutor caused* the change, and the paired t-test has no capacity to adjudicate that.\n\n---\n\n## 3. Minimally Credible Redesign (Executable Within One School Term)\n\n### Overview\n\nA **pre-registered, stratified, randomized controlled trial** with equated outcome measures, conducted within a single school term (~12\u201314 weeks).\n\n### 3.1 Pre-registration\n\n- Register the protocol, hypotheses, primary outcome, sample size justification, and analysis plan on **OSF** or **AsPredicted** before any data collection.\n- Primary hypothesis: Students assigned to the AI tutor will show greater improvement on an equated post-test than students receiving business-as-usual instruction.\n- Primary outcome: Post-test score, adjusted for pre-test score (ANCOVA).\n- Specify secondary outcomes (engagement, satisfaction) as exploratory.\n\n### 3.2 Sampling\n\n| Element | Specification |\n|---|---|\n| **Population** | All students enrolled in a specific course at a specific institution (e.g., \"Introductory Statistics, Fall 2025, University X\") |\n| **Recruitment** | Whole-class recruitment to minimize self-selection. Offer modest, equal incentives to all participants (e.g., course credit for completing pre- and post-tests). |\n| **Target N** | Power analysis: To detect a medium effect (Cohen's d = 0.5) with \u03b1 = 0.05, power = 0.80, two-tailed independent-samples test \u2192 n \u2248 64 per group, **N \u2248 128 total**. Recruit ~150 to allow for ~15% attrition. |\n| **Inclusion criteria** | Enrolled in the course, consented, completed pre-test. |\n| **Exclusion criteria** | Prior use of the AI tutor (contamination risk). |\n\n### 3.3 Randomization\n\n- **Stratified random assignment**: Stratify by pre-test score tercile (low/medium/high) and any other known prognostic variable (e.g., prior GPA). Within each stratum, randomly assign students 1:1 to treatment or control using a computer-generated sequence.\n- **Allocation concealment**: Randomization performed by a researcher not involved in instruction or assessment, after pre-test completion.\n\n#### Quasi-experimental fallback (if individual randomization is infeasible)\n\n- Use a **waitlist control** design: Randomly assign half the class sections (if multiple sections exist) to receive the tutor in weeks 1\u20136 and the other half in weeks 7\u201312. Compare outcomes at the week-6 midpoint.\n- Alternatively, use a **regression discontinuity** design if access is allocated by a cutoff (e.g., GPA), though this requires a larger sample.\n\n### 3.4 Intervention Protocol\n\n| Element | Treatment Group | Control Group |\n|---|---|---|\n| **Instruction** | Standard course + AI tutor access (structured: minimum 3 sessions/week, 30 min each, logged automatically) | Standard course only (no AI tutor access) |\n| **Duration** | 8 weeks (sufficient for meaningful exposure; avoids end-of-term confounds) |\n| **Compliance monitoring** | Log tutor usage (time, modules, interactions) to enable dose-response analysis | Log study hours via brief weekly survey (to compare total study effort) |\n| **Contamination control** | Tutor access requires individual login; control students cannot access it. Brief end-of-study survey asks control students if they used any AI tutoring tools. |\n\n### 3.5 Outcome Measurement\n\n| Element | Specification |\n|---|---|\n| **Pre-test** | Administered in week 1, before randomization. Covers course content domains. |\n| **Post-test** | Administered in week 9. **Equated** with the pre-test using one of: (a) a single test form administered at both time points (identical items\u2014risks practice effects but ensures comparability), (b) two parallel forms developed and pilot-equated via IRT on a separate calibration sample, or (c) a common-item anchor design (30% shared items, remainder unique). Option (c) is recommended. |\n| **Scoring** | IRT-based ability estimates (preferred) or raw scores on equated forms. |\n| **Blinding** | Post-tests graded by scorers blind to group assignment (for any constructed-response items). |\n| **Secondary outcomes** | Delayed retention test (week 12, 3 weeks post-intervention), self-efficacy scale, course grade. |\n\n### 3.6 Analysis Plan (Pre-registered)\n\n1. **Primary analysis**: ANCOVA with post-test score as the dependent variable, group (treatment/control) as the factor, and pre-test score as the covariate. This adjusts for baseline differences and increases power.\n2. **Effect size**: Report Cohen's d with 95% confidence interval.\n3. **Intention-to-treat**: Analyze all randomized participants regardless of compliance (to preserve randomization).\n4. **Per-protocol analysis**: Secondary analysis restricted to treatment students who met minimum usage thresholds.\n5. **Dose-response**: Exploratory regression of post-test score on logged tutor usage hours (treatment group only), controlling for pre-test.\n6. **Attrition analysis**: Compare completers vs. dropouts on pre-test scores and demographics; report differential attrition by group.\n7. **Sensitivity analyses**: Repeat primary analysis with and without outliers; test robustness to non-normality via bootstrapped CIs.\n\n### 3.7 Reporting Standards\n\n- Follow **CONSORT** guidelines (adapted for educational trials).\n- Report a flow diagram showing enrollment, randomization, allocation, follow-up, and analysis.\n- Report all pre-registered analyses regardless of significance.\n\n---\n\n## 4. What Claim Is Actually Justified by the Current Evidence\n\n> **Justified claim:** \"Among 40 self-selected volunteer students, average scores on two non-equated tests increased by 16 points over a two-week period during which students had access to our AI tutor, alongside their normal coursework and study activities.\"\n\n> **Not justified:** Any causal claim. Any claim of a specific effect size attributable to the tutor. Any claim of \"broad effectiveness.\" Any claim generalizable beyond this specific sample, subject, institution, and time period.\n\nThe current evidence is consistent with the tutor being effective. It is equally consistent with the tutor having zero effect and the observed gain being entirely explained by regression to the mean, practice effects, maturation, an easier post-test, Hawthorne effects, or some combination thereof. **The study design cannot distinguish between these explanations, and the p-value does not help.** The claim of a 25% improvement caused by the AI tutor is, at present, unsupported.",
      "input_tokens": 200,
      "output_tokens": 2919,
      "latency_ms": 4567.940252018161,
      "structural_metrics": {
        "mtld": 148.32327689156142,
        "readability_fk_grade": 12.770065889228906,
        "coherence_mean": 0.2546014213481556,
        "prompt_relevance": 0.7321559113961735,
        "connective_density": 0.12745098039215685,
        "word_count": 1727,
        "repetition_rate": 0.015081206496519721
      }
    }
  ],
  "consensus": {
    "method": "simple_vote",
    "selected_agent_id": "agent_0",
    "selected_text": "# Critical Evaluation of the AI Tutor Effectiveness Claim\n\n## 1. Methodological Threats to Validity\n\n### Threats to Internal Validity (Can we infer causation at all?)\n\n| Threat | Explanation |\n|---|---|\n| **Selection bias (self-selection)** | Volunteers are systematically different from the general student population. They are likely more motivated, more tech-savvy, or more anxious about upcoming exams\u2014all of which independently predict score improvement. The \"treatment effect\" is confounded with the traits that drove participation. |\n| **No control group** | Without a comparison group experiencing the same time period, maturation, and testing conditions, every co-occurring influence is an alternative explanation. Any improvement could be entirely due to factors unrelated to the tutor. |\n| **History / maturation** | Two weeks of ordinary studying, classroom instruction, peer discussion, and approaching-exam motivation would produce some score gain regardless of the tutor. This is indistinguishable from a tutor effect in this design. |\n| **Testing effects (practice / sensitization)** | Taking the pre-test itself is a learning event. Students become familiar with format, question types, and their own knowledge gaps. The post-test improvement partly reflects this familiarity. |\n| **Non-equivalent test forms** | The pre- and post-exams were not equated for difficulty. If the post-exam was even slightly easier, the 16-point gain is artifactually inflated. Without item-response-theory equating or a common anchor set, the two scores are not on the same scale, making the difference literally uninterpretable as a gain. |\n| **Regression to the mean** | A starting average of 64 suggests many students scored below their true ability on the pre-test (bad day, unfamiliarity, low stakes). Their scores would naturally regress upward on any retest, with or without intervention. Self-selected students who feel they are \"underperforming\" are especially susceptible to this. |\n| **Hawthorne / novelty effect** | Participants knew they were in a trial of a new product. Increased attention, effort, and engagement from novelty alone can boost performance temporarily and would vanish at scale. |\n| **Demand characteristics** | Students may have tried harder on the post-test to validate the tool they volunteered to use, or to please the researchers/company. |\n| **Attrition (unreported)** | We are told 40 students participated, but not how many initially enrolled. If weaker students dropped out, the post-test average is inflated by survivorship bias. |\n\n### Threats to External Validity (Can we generalize?)\n\n- **Sample**: 40 volunteers from (presumably) one institution, one subject, one age group. No basis for \"broad effectiveness.\"\n- **Dosage ambiguity**: \"Used the tutor for two weeks\" says nothing about hours of engagement, which modules were used, or compliance. Effect cannot be linked to a replicable treatment protocol.\n- **Ecological validity**: A short trial with motivated volunteers under observation does not predict real-world adoption where students use the tool sporadically and without monitoring.\n\n### Threats to Construct Validity\n\n- The construct \"improves exam scores\" conflates genuine learning with test-taking strategy, memorization of specific content, or simply increased study time. Without measuring transfer, retention, or deeper understanding, we don't know what changed.\n\n---\n\n## 2. Why p < 0.05 Is Insufficient for the Causal Claim\n\nThe paired t-test answers exactly one question: *\"Is the mean difference between pre- and post-scores unlikely to be zero, assuming the difference scores are drawn from a normal distribution?\"* It says nothing about **why** the difference exists.\n\n### Specific insufficiencies:\n\n1. **Statistical significance \u2260 causal identification.** The p-value quantifies sampling variability, not confounding. A paired t-test controls for between-subject variance but controls for zero confounds. Every threat listed above (history, maturation, regression, testing effects, non-equivalent forms) produces a real, non-zero mean difference that the t-test will faithfully detect. The test is doing its job correctly\u2014it's just being asked the wrong question.\n\n2. **p < 0.05 with n = 40 and a 16-point gain is unsurprising and uninformative.** A 16-point mean increase on a ~100-point scale is large. Even with substantial within-subject variability (SD of differences ~20), the t-statistic would be approximately 16/(20/\u221a40) \u2248 5.06, yielding p \u226a 0.001. The \"significance\" here is trivially achieved and tells us only that *something* changed\u2014which no one doubts. The question is *what*.\n\n3. **No effect size contextualization.** The company does not report Cohen's d, does not benchmark against typical score gains over two weeks of normal study, and does not compare to alternative interventions. A 25% relative gain sounds impressive but is uninterpretable without a counterfactual.\n\n4. **Multiple implicit comparisons.** If the company tested multiple cohorts, subjects, or time windows and reported only the most favorable result, the p-value is inflated by selection. No pre-registration is mentioned.\n\n5. **Violation of assumptions.** With non-equivalent test forms, the pre-post difference is measured in incomparable units. Running a t-test on scores from two different scales is like subtracting kilograms from pounds and testing whether the difference is zero\u2014the arithmetic is valid but the inference is meaningless.\n\n**Bottom line:** The p-value certifies that the observed difference is unlikely under the null hypothesis of zero change. But the null hypothesis of zero change was never the interesting hypothesis. The interesting hypothesis is that the *tutor caused* the change, and the paired t-test has no capacity to adjudicate that.\n\n---\n\n## 3. Minimally Credible Redesign (Executable Within One School Term)\n\n### Overview\n\nA **pre-registered, stratified, randomized controlled trial** with equated outcome measures, conducted within a single school term (~12\u201314 weeks).\n\n### 3.1 Pre-registration\n\n- Register the protocol, hypotheses, primary outcome, sample size justification, and analysis plan on **OSF** or **AsPredicted** before any data collection.\n- Primary hypothesis: Students assigned to the AI tutor will show greater improvement on an equated post-test than students receiving business-as-usual instruction.\n- Primary outcome: Post-test score, adjusted for pre-test score (ANCOVA).\n- Specify secondary outcomes (engagement, satisfaction) as exploratory.\n\n### 3.2 Sampling\n\n| Element | Specification |\n|---|---|\n| **Population** | All students enrolled in a specific course at a specific institution (e.g., \"Introductory Statistics, Fall 2025, University X\") |\n| **Recruitment** | Whole-class recruitment to minimize self-selection. Offer modest, equal incentives to all participants (e.g., course credit for completing pre- and post-tests). |\n| **Target N** | Power analysis: To detect a medium effect (Cohen's d = 0.5) with \u03b1 = 0.05, power = 0.80, two-tailed independent-samples test \u2192 n \u2248 64 per group, **N \u2248 128 total**. Recruit ~150 to allow for ~15% attrition. |\n| **Inclusion criteria** | Enrolled in the course, consented, completed pre-test. |\n| **Exclusion criteria** | Prior use of the AI tutor (contamination risk). |\n\n### 3.3 Randomization\n\n- **Stratified random assignment**: Stratify by pre-test score tercile (low/medium/high) and any other known prognostic variable (e.g., prior GPA). Within each stratum, randomly assign students 1:1 to treatment or control using a computer-generated sequence.\n- **Allocation concealment**: Randomization performed by a researcher not involved in instruction or assessment, after pre-test completion.\n\n#### Quasi-experimental fallback (if individual randomization is infeasible)\n\n- Use a **waitlist control** design: Randomly assign half the class sections (if multiple sections exist) to receive the tutor in weeks 1\u20136 and the other half in weeks 7\u201312. Compare outcomes at the week-6 midpoint.\n- Alternatively, use a **regression discontinuity** design if access is allocated by a cutoff (e.g., GPA), though this requires a larger sample.\n\n### 3.4 Intervention Protocol\n\n| Element | Treatment Group | Control Group |\n|---|---|---|\n| **Instruction** | Standard course + AI tutor access (structured: minimum 3 sessions/week, 30 min each, logged automatically) | Standard course only (no AI tutor access) |\n| **Duration** | 8 weeks (sufficient for meaningful exposure; avoids end-of-term confounds) |\n| **Compliance monitoring** | Log tutor usage (time, modules, interactions) to enable dose-response analysis | Log study hours via brief weekly survey (to compare total study effort) |\n| **Contamination control** | Tutor access requires individual login; control students cannot access it. Brief end-of-study survey asks control students if they used any AI tutoring tools. |\n\n### 3.5 Outcome Measurement\n\n| Element | Specification |\n|---|---|\n| **Pre-test** | Administered in week 1, before randomization. Covers course content domains. |\n| **Post-test** | Administered in week 9. **Equated** with the pre-test using one of: (a) a single test form administered at both time points (identical items\u2014risks practice effects but ensures comparability), (b) two parallel forms developed and pilot-equated via IRT on a separate calibration sample, or (c) a common-item anchor design (30% shared items, remainder unique). Option (c) is recommended. |\n| **Scoring** | IRT-based ability estimates (preferred) or raw scores on equated forms. |\n| **Blinding** | Post-tests graded by scorers blind to group assignment (for any constructed-response items). |\n| **Secondary outcomes** | Delayed retention test (week 12, 3 weeks post-intervention), self-efficacy scale, course grade. |\n\n### 3.6 Analysis Plan (Pre-registered)\n\n1. **Primary analysis**: ANCOVA with post-test score as the dependent variable, group (treatment/control) as the factor, and pre-test score as the covariate. This adjusts for baseline differences and increases power.\n2. **Effect size**: Report Cohen's d with 95% confidence interval.\n3. **Intention-to-treat**: Analyze all randomized participants regardless of compliance (to preserve randomization).\n4. **Per-protocol analysis**: Secondary analysis restricted to treatment students who met minimum usage thresholds.\n5. **Dose-response**: Exploratory regression of post-test score on logged tutor usage hours (treatment group only), controlling for pre-test.\n6. **Attrition analysis**: Compare completers vs. dropouts on pre-test scores and demographics; report differential attrition by group.\n7. **Sensitivity analyses**: Repeat primary analysis with and without outliers; test robustness to non-normality via bootstrapped CIs.\n\n### 3.7 Reporting Standards\n\n- Follow **CONSORT** guidelines (adapted for educational trials).\n- Report a flow diagram showing enrollment, randomization, allocation, follow-up, and analysis.\n- Report all pre-registered analyses regardless of significance.\n\n---\n\n## 4. What Claim Is Actually Justified by the Current Evidence\n\n> **Justified claim:** \"Among 40 self-selected volunteer students, average scores on two non-equated tests increased by 16 points over a two-week period during which students had access to our AI tutor, alongside their normal coursework and study activities.\"\n\n> **Not justified:** Any causal claim. Any claim of a specific effect size attributable to the tutor. Any claim of \"broad effectiveness.\" Any claim generalizable beyond this specific sample, subject, institution, and time period.\n\nThe current evidence is consistent with the tutor being effective. It is equally consistent with the tutor having zero effect and the observed gain being entirely explained by regression to the mean, practice effects, maturation, an easier post-test, Hawthorne effects, or some combination thereof. **The study design cannot distinguish between these explanations, and the p-value does not help.** The claim of a 25% improvement caused by the AI tutor is, at present, unsupported.",
    "confidence": 1.0,
    "scores": {
      "agent_0": 1.0
    },
    "metadata": {
      "votes": {
        "# critical evaluation of the ai tutor effectiveness claim ## 1. methodological threats to validity ### threats to internal validity (can we infer causation at all?) | threat | explanation | |---|---| | **selection bias (self-selection)** | volunteers are systematically different from the general student population. they are likely more motivated, more tech-savvy, or more anxious about upcoming exams\u2014all of which independently predict score improvement. the \"treatment effect\" is confounded with the traits that drove participation. | | **no control group** | without a comparison group experiencing the same time period, maturation, and testing conditions, every co-occurring influence is an alternative explanation. any improvement could be entirely due to factors unrelated to the tutor. | | **history / maturation** | two weeks of ordinary studying, classroom instruction, peer discussion, and approaching-exam motivation would produce some score gain regardless of the tutor. this is indistinguishable from a tutor effect in this design. | | **testing effects (practice / sensitization)** | taking the pre-test itself is a learning event. students become familiar with format, question types, and their own knowledge gaps. the post-test improvement partly reflects this familiarity. | | **non-equivalent test forms** | the pre- and post-exams were not equated for difficulty. if the post-exam was even slightly easier, the 16-point gain is artifactually inflated. without item-response-theory equating or a common anchor set, the two scores are not on the same scale, making the difference literally uninterpretable as a gain. | | **regression to the mean** | a starting average of 64 suggests many students scored below their true ability on the pre-test (bad day, unfamiliarity, low stakes). their scores would naturally regress upward on any retest, with or without intervention. self-selected students who feel they are \"underperforming\" are especially susceptible to this. | | **hawthorne / novelty effect** | participants knew they were in a trial of a new product. increased attention, effort, and engagement from novelty alone can boost performance temporarily and would vanish at scale. | | **demand characteristics** | students may have tried harder on the post-test to validate the tool they volunteered to use, or to please the researchers/company. | | **attrition (unreported)** | we are told 40 students participated, but not how many initially enrolled. if weaker students dropped out, the post-test average is inflated by survivorship bias. | ### threats to external validity (can we generalize?) - **sample**: 40 volunteers from (presumably) one institution, one subject, one age group. no basis for \"broad effectiveness.\" - **dosage ambiguity**: \"used the tutor for two weeks\" says nothing about hours of engagement, which modules were used, or compliance. effect cannot be linked to a replicable treatment protocol. - **ecological validity**: a short trial with motivated volunteers under observation does not predict real-world adoption where students use the tool sporadically and without monitoring. ### threats to construct validity - the construct \"improves exam scores\" conflates genuine learning with test-taking strategy, memorization of specific content, or simply increased study time. without measuring transfer, retention, or deeper understanding, we don't know what changed. --- ## 2. why p < 0.05 is insufficient for the causal claim the paired t-test answers exactly one question: *\"is the mean difference between pre- and post-scores unlikely to be zero, assuming the difference scores are drawn from a normal distribution?\"* it says nothing about **why** the difference exists. ### specific insufficiencies: 1. **statistical significance \u2260 causal identification.** the p-value quantifies sampling variability, not confounding. a paired t-test controls for between-subject variance but controls for zero confounds. every threat listed above (history, maturation, regression, testing effects, non-equivalent forms) produces a real, non-zero mean difference that the t-test will faithfully detect. the test is doing its job correctly\u2014it's just being asked the wrong question. 2. **p < 0.05 with n = 40 and a 16-point gain is unsurprising and uninformative.** a 16-point mean increase on a ~100-point scale is large. even with substantial within-subject variability (sd of differences ~20), the t-statistic would be approximately 16/(20/\u221a40) \u2248 5.06, yielding p \u226a 0.001. the \"significance\" here is trivially achieved and tells us only that *something* changed\u2014which no one doubts. the question is *what*. 3. **no effect size contextualization.** the company does not report cohen's d, does not benchmark against typical score gains over two weeks of normal study, and does not compare to alternative interventions. a 25% relative gain sounds impressive but is uninterpretable without a counterfactual. 4. **multiple implicit comparisons.** if the company tested multiple cohorts, subjects, or time windows and reported only the most favorable result, the p-value is inflated by selection. no pre-registration is mentioned. 5. **violation of assumptions.** with non-equivalent test forms, the pre-post difference is measured in incomparable units. running a t-test on scores from two different scales is like subtracting kilograms from pounds and testing whether the difference is zero\u2014the arithmetic is valid but the inference is meaningless. **bottom line:** the p-value certifies that the observed difference is unlikely under the null hypothesis of zero change. but the null hypothesis of zero change was never the interesting hypothesis. the interesting hypothesis is that the *tutor caused* the change, and the paired t-test has no capacity to adjudicate that. --- ## 3. minimally credible redesign (executable within one school term) ### overview a **pre-registered, stratified, randomized controlled trial** with equated outcome measures, conducted within a single school term (~12\u201314 weeks). ### 3.1 pre-registration - register the protocol, hypotheses, primary outcome, sample size justification, and analysis plan on **osf** or **aspredicted** before any data collection. - primary hypothesis: students assigned to the ai tutor will show greater improvement on an equated post-test than students receiving business-as-usual instruction. - primary outcome: post-test score, adjusted for pre-test score (ancova). - specify secondary outcomes (engagement, satisfaction) as exploratory. ### 3.2 sampling | element | specification | |---|---| | **population** | all students enrolled in a specific course at a specific institution (e.g., \"introductory statistics, fall 2025, university x\") | | **recruitment** | whole-class recruitment to minimize self-selection. offer modest, equal incentives to all participants (e.g., course credit for completing pre- and post-tests). | | **target n** | power analysis: to detect a medium effect (cohen's d = 0.5) with \u03b1 = 0.05, power = 0.80, two-tailed independent-samples test \u2192 n \u2248 64 per group, **n \u2248 128 total**. recruit ~150 to allow for ~15% attrition. | | **inclusion criteria** | enrolled in the course, consented, completed pre-test. | | **exclusion criteria** | prior use of the ai tutor (contamination risk). | ### 3.3 randomization - **stratified random assignment**: stratify by pre-test score tercile (low/medium/high) and any other known prognostic variable (e.g., prior gpa). within each stratum, randomly assign students 1:1 to treatment or control using a computer-generated sequence. - **allocation concealment**: randomization performed by a researcher not involved in instruction or assessment, after pre-test completion. #### quasi-experimental fallback (if individual randomization is infeasible) - use a **waitlist control** design: randomly assign half the class sections (if multiple sections exist) to receive the tutor in weeks 1\u20136 and the other half in weeks 7\u201312. compare outcomes at the week-6 midpoint. - alternatively, use a **regression discontinuity** design if access is allocated by a cutoff (e.g., gpa), though this requires a larger sample. ### 3.4 intervention protocol | element | treatment group | control group | |---|---|---| | **instruction** | standard course + ai tutor access (structured: minimum 3 sessions/week, 30 min each, logged automatically) | standard course only (no ai tutor access) | | **duration** | 8 weeks (sufficient for meaningful exposure; avoids end-of-term confounds) | | **compliance monitoring** | log tutor usage (time, modules, interactions) to enable dose-response analysis | log study hours via brief weekly survey (to compare total study effort) | | **contamination control** | tutor access requires individual login; control students cannot access it. brief end-of-study survey asks control students if they used any ai tutoring tools. | ### 3.5 outcome measurement | element | specification | |---|---| | **pre-test** | administered in week 1, before randomization. covers course content domains. | | **post-test** | administered in week 9. **equated** with the pre-test using one of: (a) a single test form administered at both time points (identical items\u2014risks practice effects but ensures comparability), (b) two parallel forms developed and pilot-equated via irt on a separate calibration sample, or (c) a common-item anchor design (30% shared items, remainder unique). option (c) is recommended. | | **scoring** | irt-based ability estimates (preferred) or raw scores on equated forms. | | **blinding** | post-tests graded by scorers blind to group assignment (for any constructed-response items). | | **secondary outcomes** | delayed retention test (week 12, 3 weeks post-intervention), self-efficacy scale, course grade. | ### 3.6 analysis plan (pre-registered) 1. **primary analysis**: ancova with post-test score as the dependent variable, group (treatment/control) as the factor, and pre-test score as the covariate. this adjusts for baseline differences and increases power. 2. **effect size**: report cohen's d with 95% confidence interval. 3. **intention-to-treat**: analyze all randomized participants regardless of compliance (to preserve randomization). 4. **per-protocol analysis**: secondary analysis restricted to treatment students who met minimum usage thresholds. 5. **dose-response**: exploratory regression of post-test score on logged tutor usage hours (treatment group only), controlling for pre-test. 6. **attrition analysis**: compare completers vs. dropouts on pre-test scores and demographics; report differential attrition by group. 7. **sensitivity analyses**: repeat primary analysis with and without outliers; test robustness to non-normality via bootstrapped cis. ### 3.7 reporting standards - follow **consort** guidelines (adapted for educational trials). - report a flow diagram showing enrollment, randomization, allocation, follow-up, and analysis. - report all pre-registered analyses regardless of significance. --- ## 4. what claim is actually justified by the current evidence > **justified claim:** \"among 40 self-selected volunteer students, average scores on two non-equated tests increased by 16 points over a two-week period during which students had access to our ai tutor, alongside their normal coursework and study activities.\" > **not justified:** any causal claim. any claim of a specific effect size attributable to the tutor. any claim of \"broad effectiveness.\" any claim generalizable beyond this specific sample, subject, institution, and time period. the current evidence is consistent with the tutor being effective. it is equally consistent with the tutor having zero effect and the observed gain being entirely explained by regression to the mean, practice effects, maturation, an easier post-test, hawthorne effects, or some combination thereof. **the study design cannot distinguish between these explanations, and the p-value does not help.** the claim of a 25% improvement caused by the ai tutor is, at present, unsupported.": 1
      }
    }
  },
  "topology": {
    "name": "flat",
    "rounds": 1,
    "metadata": {}
  },
  "debate_rounds": [],
  "evaluation": {
    "quality_score": 0.5,
    "corrected_metrics": {
      "consensus_candidate_id": "final_consensus",
      "consensus_win_rate": 0.5,
      "normalized_bt_score": 1.0,
      "num_bt_candidates": 2,
      "consensus_vs_best_agent": true,
      "consensus_comparisons": 1,
      "per_judge_consensus_win_rate": {
        "claude-sonnet-4-6": 0.5,
        "deepseek-v3p2": 0.5,
        "gpt-4o": 0.5
      }
    },
    "threshold_met": null,
    "thresholds": {},
    "selected_per_judge_scores": {
      "claude-sonnet-4-6": 0.5,
      "gpt-4o": 0.5,
      "deepseek-v3p2": 0.5
    },
    "judge_panel": {
      "judges": [
        "claude-sonnet-4-6",
        "gpt-4o",
        "deepseek-v3p2"
      ],
      "bt_scores": {
        "final_consensus": 0.5,
        "agent_0": 0.5
      },
      "ranking": [
        "final_consensus",
        "agent_0"
      ],
      "per_judge_bt_scores": {
        "claude-sonnet-4-6": {
          "final_consensus": 0.5,
          "agent_0": 0.5
        },
        "gpt-4o": {
          "final_consensus": 0.5,
          "agent_0": 0.5
        },
        "deepseek-v3p2": {
          "final_consensus": 0.5,
          "agent_0": 0.5
        }
      },
      "inter_rater_reliability": {
        "mean_cohen_kappa": 1.0,
        "pairwise": {
          "claude-sonnet-4-6__vs__deepseek-v3p2": 1.0,
          "claude-sonnet-4-6__vs__gpt-4o": 1.0,
          "deepseek-v3p2__vs__gpt-4o": 1.0
        }
      },
      "pairwise_records": [
        {
          "candidate_i": "final_consensus",
          "candidate_j": "agent_0",
          "per_judge": {
            "claude-sonnet-4-6": "tie",
            "gpt-4o": "tie",
            "deepseek-v3p2": "tie"
          },
          "majority_winner": null
        }
      ]
    },
    "disagreement": {
      "semantic_pairwise_similarity": 1.0,
      "disagreement_rate": 0.0,
      "vote_entropy": 0.0,
      "lexical_diversity": 0.5100171722953635,
      "pairwise_similarity_lexical": 1.0
    },
    "structural_metrics": {
      "mtld": 148.32327689156142,
      "readability_fk_grade": 12.770065889228906,
      "coherence_mean": 0.2546014213481556,
      "prompt_relevance": 0.7321559113961735,
      "connective_density": 0.12745098039215685,
      "word_count": 1727,
      "repetition_rate": 0.015081206496519721
    },
    "human_review": {
      "flagged": false,
      "reasons": [],
      "sheet_path": null
    }
  }
}